<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Bijay Gaudel">
    <meta name="description" content="By the end of this post you will learn to:
 Estimate action values using the sample average method. Describe greedy action selection. Introduce the exploration-exploitation dilemma. Describe how action values can be estimated incrementally using the sample-average method. Identify how the incremental update rule is an instance of a more general update rule. Describe how the general update rule can be used to solve a non-stationary bandit problem.  Value of Action The value of action is the expected reward when the action is taken.">
    <meta name="keywords" content="artificial intelligence, blog, machine learning, deep learning, reinforcement learning">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Estimating Action Values"/>
<meta name="twitter:description" content="By the end of this post you will learn to:
 Estimate action values using the sample average method. Describe greedy action selection. Introduce the exploration-exploitation dilemma. Describe how action values can be estimated incrementally using the sample-average method. Identify how the incremental update rule is an instance of a more general update rule. Describe how the general update rule can be used to solve a non-stationary bandit problem.  Value of Action The value of action is the expected reward when the action is taken."/>

    <meta property="og:title" content="Estimating Action Values" />
<meta property="og:description" content="By the end of this post you will learn to:
 Estimate action values using the sample average method. Describe greedy action selection. Introduce the exploration-exploitation dilemma. Describe how action values can be estimated incrementally using the sample-average method. Identify how the incremental update rule is an instance of a more general update rule. Describe how the general update rule can be used to solve a non-stationary bandit problem.  Value of Action The value of action is the expected reward when the action is taken." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gaudelbijay.github.io/blogs/estimating_action_values/" /><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2020-09-28T17:04:37&#43;05:45" />
<meta property="article:modified_time" content="2020-09-28T17:04:37&#43;05:45" />



    
      <base href="https://gaudelbijay.github.io/blogs/estimating_action_values/">
    
    <title>
  Estimating Action Values Â· Bijay Gaudel
</title>

    
      <link rel="canonical" href="https://gaudelbijay.github.io/blogs/estimating_action_values/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://gaudelbijay.github.io/css/coder.min.fce3d3684e836ce340cdd591dad1749c310472d5ed4049fab99845c9aec4d4bf.css" integrity="sha256-/OPTaE6DbONAzdWR2tF0nDEEctXtQEn6uZhFya7E1L8=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="https://gaudelbijay.github.io/css/coder-dark.min.717236c74e0a5208ef73964a9f44c6b443b689a95b270d8b2a40d0c012460dac.css" integrity="sha256-cXI2x04KUgjvc5ZKn0TGtEO2ialbJw2LKkDQwBJGDaw=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="https://gaudelbijay.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://gaudelbijay.github.io/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="https://gaudelbijay.github.io/images/apple-touch-icon.png">
    <link rel="apple-touch-icon"  sizes="180x180" href="https://gaudelbijay.github.io/images/apple-touch-icon.png">

    <meta name="generator" content="Hugo 0.83.1" />
  </head>
  
  
  
    
  
  <body class="colorscheme-auto"
        onload=" twemoji.parse(document.body); "
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://gaudelbijay.github.io/">
      Bijay Gaudel
    </a>
    
      <span id="dark-mode-toggle" class="float-right">
        <i class="fas fa-adjust fa-fw"></i>
      </span>
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fas fa-bars fa-fw"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gaudelbijay.github.io/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gaudelbijay.github.io/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gaudelbijay.github.io/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gaudelbijay.github.io/quotes/">Quotes</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gaudelbijay.github.io/contact/">Contact me</a>
            </li>
          
        
        
        <li class="navigation-item separator">
          <span>|</span>
        </li>
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Estimating Action Values</h1>
    </header>

    <p>By the end of this post you will learn to:</p>
<ul>
<li>Estimate action values using the sample average method.</li>
<li>Describe greedy action selection.</li>
<li>Introduce the exploration-exploitation dilemma.</li>
<li>Describe how action values can be estimated incrementally using the sample-average method.</li>
<li>Identify how the incremental update rule is an instance of a more general update rule.</li>
<li>Describe how the general update rule can be used to solve a non-stationary bandit problem.</li>
</ul>
<h4 id="value-of-action">Value of Action</h4>
<p>The value of action is the expected reward when the action is taken.</p>
<p>Mathematically,</p>
<blockquote>
<p>$$q_{*}(a)=\mathbb{E}[R_t|A_t=a]$$</p>
</blockquote>
<p>$q_*(a)$ is not known, so we estimate it.</p>
<h3 id="way-to-estimate-q_">Way to Estimate $q_*$</h3>
<ul>
<li>
<h4 id="sample-average-method">Sample-Average Method:</h4>
</li>
</ul>
<p>$$Q_t(a)=\frac{sum of rewards when \alpha (action) taken prior to t}{number of times \alpha (action) taken prior to t}$$</p>
<p>$$Q_t(a)=\frac{\sum_{i}^{t-1}R_i}{t-1}$$</p>
<h6 id="example-clinical-trials">Example: Clinical Trials</h6>
<p><img src="https://gaudelbijay.github.io/img/Estimating_action_values/medical_trail.PNG" alt="image alt text">
Lets say a doctor is learning to select the best treatment for different patient with same disease and gives a treatment randomly. Out of 4 trails on treatment $Y$, 3 trails gives positive result. Next time, if a patient with same symptoms come to see the doctor, doctor is likely to give treatment $Y$ and update the estimation value for $Y$. After updating estimation value doctor will take the treatment with best estimation value in next treatment. In this way doctor learn to give treatment, and this the example of greedy action. In reality doctor would not randomly select treatment to the patient instead they would probably assgin a treatment that they think is the best.</p>
<p>Selecting a greedy action means the agent is exploiting its current result, it is trying to get the maximum result right now as it can. We can select the greedy action by taking the argmax of estimated value.</p>
<p>$$\alpha=argmax({Q_P, Q_Y, Q_B})$$</p>
<h6 id="exploiting-vs-exploring">Exploiting Vs Exploring</h6>
<p>Agent also can choose non greedy action to explore more about the effect of other treatment this is the one of the most confusing part of reinforcement learning, this is called exploiting-exploring dilemma.</p>
<h5 id="incremental-rule">Incremental Rule</h5>
<p>$$Q_{n+1}=\frac{1}{n}\sum_{i=1}^{n}R_i$$
$$Q_{n+1}=(R_n+\sum_{i=1}^{n-1}R_i)$$
$$Q_{n+1}=\frac{1}{n}((n+1)\frac{1}{(n-1)}\sum_{i=1}^{n-1}R_i)$$
$$Q_{n+1}=\frac{1}{n}(R_n+(n-1)Q_n)$$
$$Q_{n+1}=\frac{1}{n}(R_n+nQ_n-Q_n)$$
$$Q_{n+1}=Q_n+\frac{1}{n}(R_n-Q_n)$$
$$Q_{n+1}=Q_n+\alpha_n(R_n-Q_n)$$</p>
<p>The above derivation can be conclude by:</p>
<blockquote>
<p>New Estimate = Old Estimate + Setp Size*(target - Old Estimate)</p>
</blockquote>
<p>Step size can be a function of n that produces a number from 0 to 1.
i.e. $\alpha_n=[0,1]$.</p>
<p>In the specific case of the sample average, the step size is equal to the $\frac{1}{n}$
$$\alpha=\frac{1}{n}$$</p>
<p>Lets go to our friendly doctor example,</p>
<p>What if one of the treatment is more effective under certain condition. Lets say treatment $B$ is more effective during winter months. This is the sample of non stationary bandit problem. Distributionof rewards changes with time. The Docotr is unaware of this change but would like to adapt it.
One step to solve this problem is fixed step size, if $\alpha_n$ is fixed the most recent reward affect the estimatie more than other rewards.
<img src="https://gaudelbijay.github.io/img/Estimating_action_values/reward_weights_n.PNG" alt="image alt text"></p>
<p>This graph shows the most recent reward received versus the reward received $t$ time steps ago.</p>
<h5 id="decaying-past-rewards">Decaying Past Rewards</h5>
<p>As we know
$$Q_{n+1}=Q_n+\alpha_n(R_n-Q_n)$$</p>
<p>$$Q_{n+1}=\alpha R_n+Q_n-\alpha Q_n$$</p>
<p>$$Q_{n+1}=\alpha R_n+(1-\alpha)Q_n$$</p>
<p>$$Q_{n+1}=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]$$</p>
<p>$$Q_{n+1}=Q_nR_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2\alpha R_{n-2}+ \cdots +(1-\alpha)^{n-1}\alpha R_1+(1-\alpha)^nQ_1$$
&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.
$$Q_{n+1}=(1-\alpha)^nQ_1+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_i$$</p>
<p>This equation relates our current estimation values $Q_{n+1}$ to $Q_1$ and all observed rewards.
The first term tells us that first estimation decreases exponentially with time and second term tells us the reward further back in time contibute exponentially less to the sum.</p>
<p>Generalization of this equation shows that the value of $Q_1$ goes to zero as we go by more and more data and most recent rewards contribute most to the current estimation.</p>
<h5 id="summary">Summary</h5>
<ul>
<li>Sample-average method can be used to estimate action values.</li>
<li>The greedy action is the action with the higher value estimate.</li>
<li>Derived incremental sample-average method.</li>
<li>Generalize the incremental update rule into a more general update rule.</li>
<li>A constant stepsize paramters can be used to solve a non stationary bandit problem.</li>
</ul>
<h6 id="refrences">Refrences:</h6>
<ol>
<li><a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback">https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback</a></li>
<li><a href="http://www.damiankolmas.com/rl/Action-value-method/">http://www.damiankolmas.com/rl/Action-value-method/</a></li>
</ol>

  </article>
</section>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );">
  </script>

      </div>

      
    </main>

    
      
      <script src="https://gaudelbijay.github.io/js/dark-mode.min.aee9c8a464eb7b3534c7110f7c5e169e7039e2fd92710e0626d451d6725af137.js"></script>
    

    

    

    <script>
(function(f, a, t, h, o, m){
	a[h]=a[h]||function(){
		(a[h].q=a[h].q||[]).push(arguments)
	};
	o=f.createElement('script'),
	m=f.getElementsByTagName('script')[0];
	o.async=1; o.src=t; o.id='fathom-script';
	m.parentNode.insertBefore(o,m)
})(document, window, '//analytics.example.com/tracker.js', 'fathom');
fathom('set', 'siteId', 'ABCDE');
fathom('trackPageview');
</script>


    <script async defer data-domain="example.com" src="https://analytics.example.com/js/plausible.js"></script>


  </body>

</html>
